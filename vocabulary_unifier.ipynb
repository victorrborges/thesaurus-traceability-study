{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "expand-substantivos.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNWbedVD+mVZgxWXMBkpJ5u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victorrborges/thesaurus-traceability-study/blob/main/vocabulary_unifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIUh4aPwrMB_",
        "outputId": "bae8cf7a-1f52-4aac-a4ea-922f35e97505",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vZiO8QHrQDf",
        "outputId": "786f1ced-a91b-4e88-e92b-f15c2ee32596",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import requests\n",
        "import json\n",
        "\n",
        "from nltk.corpus import wordnet \n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "conceptnet_syns = {}\n",
        "conceptnet_check = {}\n",
        "\n",
        "tcs_csv = pd.read_csv('https://gist.githubusercontent.com/victorrborges/c12f4f21d3774505ec4b21976d5c29cd/raw/bea8ea328a1cf5f2ff5fcca3438b0f2cdb137055/testcases_final.csv', \",\")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7IlnaJEuYZT"
      },
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "stop_words = stopwords.words('english')\n",
        "def tokenizeDoc(string):\n",
        "  word_tokens = tokenizer.tokenize(string)\n",
        "  filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "  filtered_sentence = [token.lower() for token in filtered_sentence]\n",
        "  return filtered_sentence"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2kdfeWsKHo_"
      },
      "source": [
        "wordFreq = {}\n",
        "\n",
        "def countWordFreq(words_token):\n",
        "  for token in words_token:\n",
        "    if token.isalpha():\n",
        "      if token not in wordFreq:\n",
        "          wordFreq[token] = 0 \n",
        "      wordFreq[token] += 1\n",
        "    \n",
        "\n",
        "for index, row in tcs_csv.iterrows():\n",
        "  tokens = tokenizeDoc(row['tc_desc'])\n",
        "  countWordFreq(tokens)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMT087XUuw4F"
      },
      "source": [
        "nouns = {x.name().split('.', 1)[0] for x in wordnet.all_synsets('n')}\n",
        "computer_science_relations = ['computing', 'computer programming', 'computation', 'computer', 'software',  'code', 'programming', 'programming language', 'program']\n",
        "\n",
        "def expandDoc(words_token, n, api, technical_terms_conditional, nouns_conditional):\n",
        "  expandedDoc = []\n",
        "  for token in words_token:\n",
        "    if token.isalpha() and shouldExpandTerm(token, technical_terms_conditional, nouns_conditional):\n",
        "      finalTokenExpanded = list(set([token] + getTopSynsets(getSynonyms(token, api), n)))\n",
        "      expandedDoc += finalTokenExpanded\n",
        "    else:\n",
        "      expandedDoc += [token]\n",
        "  return expandedDoc\n",
        "\n",
        "def shouldExpandTerm(token, technical_terms_conditional, nouns_conditional):\n",
        "  shouldExpand = True\n",
        "  \n",
        "  first_check = nouns_conditional and token not in nouns\n",
        "  second_check = technical_terms_conditional and not isRelatedToComputerScience(token)\n",
        "  \n",
        "  if first_check or second_check:\n",
        "    shouldExpand = False\n",
        "\n",
        "  return shouldExpand\n",
        "\n",
        "def isRelatedToComputerScience(word):\n",
        "  if word not in conceptnet_check:\n",
        "    url = \"http://api.conceptnet.io/query?start=/c/en/\" + word + \"&rel=/r/RelatedTo&end=/c/en\"\n",
        "    relationsNodeList = requests.get(url).json()['edges']\n",
        "    relationsList = [word['end']['label'] for word in relationsNodeList]\n",
        "    check =  any(item in relationsList for item in computer_science_relations)\n",
        "    conceptnet_check[word] = check\n",
        "    return check\n",
        "  else:\n",
        "    return conceptnet_check[word]\n",
        "\n",
        "def getTopSynsets(words, n):\n",
        "  words = list(set(words))\n",
        "  topN = []\n",
        "  wordsFrequency = {}\n",
        "  for word in words:\n",
        "    if word in wordFreq:\n",
        "      wordsFrequency[word] = wordFreq[word]\n",
        "  wordsFrequency = sorted(wordsFrequency.items(), key=lambda item: item[1], reverse=True)\n",
        "  wordsFrequency = [word[0] for word in wordsFrequency]\n",
        "  return wordsFrequency[:n]\n",
        "\n",
        "def getSynonyms(token, api):\n",
        "  if api == 'wordnet':\n",
        "    return wordNetSynonyms(token)\n",
        "  else:\n",
        "    return conceptNetSynonyms(token)\n",
        "\n",
        "def conceptNetSynonyms(word):\n",
        "  if word not in conceptnet_syns:\n",
        "    url = \"http://api.conceptnet.io/query?start=/c/en/\" + word + \"&rel=/r/Synonym&end=/c/en\"\n",
        "    synonymNodeList = requests.get(url).json()['edges']\n",
        "    synonymList = [word['end']['label'] for word in synonymNodeList]\n",
        "    conceptnet_syns[word] = synonymList\n",
        "    return synonymList\n",
        "  else:\n",
        "    return conceptnet_syns[word]\n",
        "\n",
        "def wordNetSynonyms(token):\n",
        "  tokenExpanded = []\n",
        "  for syn in wordnet.synsets(token):\n",
        "    tokenExpanded += syn.lemma_names()\n",
        "  return tokenExpanded"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-DYEi8c1OT6"
      },
      "source": [
        "# SET PARAMETERS:\n",
        "api = 'conceptnet' #'wordnet' # or 'conceptnet'\n",
        "technical_terms_conditional = True\n",
        "nouns_conditional = True\n",
        "\n",
        "def getCSVLabel(limit):\n",
        "  label = '{}_expanded_'.format(api)\n",
        "  if nouns_conditional:\n",
        "    label += 'nouns_'\n",
        "  if technical_terms_conditional:\n",
        "    label += 'related_'\n",
        "  label += 'top_{}_selected_bugreports_final.csv'.format(limit)\n",
        "  return label\n",
        "\n",
        "for i in range(1, 6):\n",
        "  limit = i\n",
        "  bug_csv = pd.read_csv('https://gist.githubusercontent.com/victorrborges/d5a7b41c1fb61608b6c4e7d91be69517/raw/f4e8ec20adad82634a5d5e4be66b4ea0c5cfb69e/selected_bugreports_final.csv', \",\")\n",
        "\n",
        "  for index, row in bug_csv.iterrows():\n",
        "    tokens = tokenizeDoc(row['br_desc'])\n",
        "    expanded_br_desc = \" \".join(expandDoc(tokens, limit, api, technical_terms_conditional, nouns_conditional))\n",
        "    bug_csv.at[index, 'br_desc'] =  expanded_br_desc\n",
        "\n",
        "  bug_csv.to_csv(getCSVLabel(limit), index=False)"
      ],
      "execution_count": 71,
      "outputs": []
    }
  ]
}